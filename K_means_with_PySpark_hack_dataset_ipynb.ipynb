{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Ek5-z6b1OXuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2c8fea-7291-43df-cb8c-22b445c30cd4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXctX0cEQuIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2965d851-aeb9-4c6b-a032-dd040e44bebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |█████████████████▉              | 156.3 MB 1.3 MB/s eta 0:01:40"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEcARX8LRwLi"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRzShXx3SCFo"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"num_of_hackers\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3bnGk_zSHr6"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv(\"/content/drive/MyDrive/PySpark/hack_data.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDcwz51ASPAB"
      },
      "outputs": [],
      "source": [
        "df.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pptJkQQKSS3_"
      },
      "outputs": [],
      "source": [
        "round(df.describe().toPandas(), 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foP_dVMeSZfB"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB_-KaRLStxf"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhKT3L2jSwCS"
      },
      "outputs": [],
      "source": [
        "feat_cols = ['Session_Connection_Time',\n",
        " 'Bytes Transferred',\n",
        " 'Kali_Trace_Used',\n",
        " 'Servers_Corrupted',\n",
        " 'Pages_Corrupted',\n",
        " 'WPM_Typing_Speed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9CdFaaZS0pm"
      },
      "outputs": [],
      "source": [
        "assembler = VectorAssembler(inputCols=feat_cols, outputCol='features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nmV9-YUS5oN"
      },
      "outputs": [],
      "source": [
        "final_df = assembler.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOt2lG0_S9H9"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VRieMVeTEF_"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler(inputCol='features', \n",
        "                        outputCol='scaled_feat',\n",
        "                        withStd = True,\n",
        "                        withMean = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUb1fkh9TPwQ"
      },
      "outputs": [],
      "source": [
        "scaled_model = scaler.fit(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wbR8VwQTWNh"
      },
      "outputs": [],
      "source": [
        "cluster_df = scaled_model.transform(final_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9TwASEfTcQf"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kURhp5x0Tiwn"
      },
      "outputs": [],
      "source": [
        "eval = ClusteringEvaluator(predictionCol=\"prediction\",\n",
        "                           featuresCol=\"scaled_feat\",\n",
        "                           metricName=\"silhouette\",\n",
        "                           distanceMeasure=\"squaredEuclidean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keIi0Fe-T-lr"
      },
      "outputs": [],
      "source": [
        "silhouette_score = []\n",
        "print(\"\"\"\n",
        "Silhoutte Scores for K Mean Clustering\n",
        "======================================\n",
        "Model\\tScore\\t\n",
        "=====\\t=====\\t\n",
        "\"\"\")\n",
        "for k in range(2,11):\n",
        "  kmeans_algo = KMeans(featuresCol='scaled_feat',k=k)\n",
        "  kmeans_fit = kmeans_algo.fit(cluster_df)\n",
        "  output = kmeans_fit.transform(cluster_df)\n",
        "  score = eval.evaluate(output)\n",
        "  silhouette_score.append(score)\n",
        "  print(f\"K{k}\\t{round(score,2)}\\t\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bAzw9HVU5wt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10,10))\n",
        "ax.plot(range(2,11), silhouette_score)\n",
        "ax.set_xlabel(\"K\")\n",
        "ax.set_ylabel(\"Score\");"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "K-means with PySpark - hack dataset.ipynb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}